# import base64
from openai import OpenAI
import anthropic
from ..logger import get_custom_logger

# import fitz

logger = get_custom_logger("completions")


class TextStreamingFactory:
    client = None
    max_tokens = 4096
    attachments = []

    def __init__(
        self,
        provider: str,
        api_key: str,
        config: dict = {},
        prev_messages=[],
        agent_slug=None,
    ) -> None:
        if provider == "openai":
            self.client = OpenAI(api_key=api_key)
        elif provider == "ollama":
            self.client = OpenAI(api_key=api_key, base_url="http://localhost:11434/v1")
        elif provider == "anthropic":
            self.client = anthropic.Anthropic(api_key=api_key)

        elif provider == "xai":
            self.client = OpenAI(api_key=api_key, base_url="https://api.x.ai/v1")

        self.provider = provider

        self.prev_messages = prev_messages
        self.config = config
        self.agent_slug = agent_slug

    def stream(self, system: str, text: str, model: str):

        if self.config.get("multiagentic_modality", "isolated") == "grupal":
            system = f"{system}\n\nYou must consider that the other AI in the conversation is another user and refer to them as such. ONLY GIVE YOUR RESPONSE."

        if self.provider in ["openai", "ollama", "xai"]:
            return self.stream_openai(system, text, model)
        elif self.provider == "anthropic":
            print("trying to generate with Anthropic")
            return self.stream_anthropic(system, text, model)

        print("PROVIDER NOT FOUND")

    def stream_openai(self, system: str, text: str, model: str):
        reasoning_models = ["o1-mini", "o1-preview", "o3-mini", "o4-mini", "o3"]
        is_reasoning_model = model in reasoning_models

        if is_reasoning_model:
            self.max_tokens = 16000

        messages = [
            {"role": "system" if not is_reasoning_model else "user", "content": system},
        ]
        for m in self.prev_messages:
            if m["type"] == "user":
                messages.append({"role": "user", "content": m["text"]})
            elif m["type"] == "assistant":
                multiagentic_modality = self.config.get(
                    "multiagentic_modality", "isolated"
                )

                if multiagentic_modality == "grupal":

                    for v in m["versions"]:
                        if v["agent_slug"] == self.agent_slug:
                            messages.append({"role": "assistant", "content": v["text"]})

                        else:
                            tagged_text = f"This message was generated by other AI in the conversation called: {v['agent_name']} \n\n AI Response: {v['text']}"

                            messages.append({"role": "user", "content": tagged_text})

                else:

                    found_version = next(
                        (
                            item
                            for item in m["versions"]
                            if item["agent_slug"] == self.agent_slug
                        ),
                        None,
                    )
                    if found_version:
                        messages.append(
                            {"role": "assistant", "content": found_version["text"]}
                        )
            elif m["type"] == "prev_ai":
                prev_ai_context = f"""
## Message Type: {m["type"]}

## Explanation
This message was generated by ANOTHER AI previous to you as an answer to the user message. You can this discuss or use this information to generate a better response depending of the conversation context.

## Details
AI_NAME:  {m["agent_name"]}
AI_RESPONSE: 
{m["text"]}
"""

                messages.append({"role": "user", "content": prev_ai_context})

        messages.append({"role": "user", "content": text})

        if len(self.attachments) > 0:
            for a in self.attachments:
                if "image" in a["type"]:
                    logger.debug("Appending image to messages")
                    messages.append(
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "image_url",
                                    "image_url": {"url": a["content"]},
                                }
                            ],
                        },
                    )
                else:
                    if "audio" in a["type"]:
                        logger.debug("Skipping audio file")
                        continue
                        if a["content"].startswith("data:audio/"):
                            audio_data = a["content"].split(",")[1]
                        else:
                            audio_data = a["content"]

                        messages.append(
                            {
                                "role": "user",
                                "content": [
                                    {
                                        "type": "text",
                                        "text": "This is an user recording",
                                    },
                                    {
                                        "type": "input_audio",
                                        "input_audio": {
                                            "data": audio_data,
                                            "format": "wav",
                                        },
                                    },
                                ],
                            },
                        )

        temperature = float(self.config.get("temperature", 1))
        if is_reasoning_model:
            logger.debug("Using reasoning model with temperature 1")
            temperature = 1

        logger.debug(f"Creating completion with {model}")
        response = self.client.chat.completions.create(
            model=model,
            max_completion_tokens=int(self.config.get("max_tokens", 3000)),
            messages=messages,
            frequency_penalty=float(self.config.get("frequency_penalty", 0)),
            temperature=temperature,
            top_p=float(self.config.get("top_p", 1.0)),
            presence_penalty=float(self.config.get("presence_penalty", 0)),
            stream=True,
            stream_options={"include_usage": True},
        )
        for chunk in response:
            try:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
            except Exception:
                yield chunk.usage

    def stream_anthropic(self, system: str, text: str, model: str):
        messages = []
        multiagentic_modality = self.config.get("multiagentic_modality", "isolated")

        for m in self.prev_messages:
            if m["type"] == "user":
                messages.append({"role": "user", "content": m["text"]})

            elif m["type"] == "assistant":
                multiagentic_modality = self.config.get(
                    "multiagentic_modality", "isolated"
                )

                if multiagentic_modality == "grupal":

                    for v in m["versions"]:
                        if v["agent_slug"] == self.agent_slug:
                            messages.append({"role": "assistant", "content": v["text"]})

                        else:
                            tagged_text = f"This message was generated by other AI in the conversation called: {v['agent_name']} \n\n AI Response: {v['text']}"

                            messages.append({"role": "user", "content": tagged_text})

                else:

                    found_version = next(
                        (
                            item
                            for item in m["versions"]
                            if item["agent_slug"] == self.agent_slug
                        ),
                        None,
                    )
                    if found_version:
                        messages.append(
                            {"role": "assistant", "content": found_version["text"]}
                        )

            elif m["type"] == "prev_ai":
                prev_ai_context = f"""
## Message Type: {m["type"]}

## Explanation
This message was generated by ANOTHER AI previous to you as an answer to the user message. You can this discuss or use this information to generate a better response depending of the conversation context.

## Details
AI_NAME:  {m["agent_name"]}
AI_RESPONSE: 
---
{m["text"]}
---
"""
                messages.append({"role": "user", "content": prev_ai_context})

        messages.append({"role": "user", "content": text})

        with self.client.messages.stream(
            max_tokens=self.max_tokens,
            system=system,
            messages=messages,
            model=model,
            temperature=self.config.get("temperature", 0.5),
        ) as stream:

            for text in stream.text_stream:
                yield text

    def process_attachments(self, attachments=[]):
        logger.debug(f"Processing {len(attachments)} attachments")

        processed = []

        for a in attachments:
            if "image" in a["type"]:
                processed.append(a)

            # elif "audio" in a["type"]:
            #     processed.append(a)
            # elif "pdf" in a["type"]:
            #     base64_content = a["content"]
            #     if base64_content.startswith("data:application/pdf;base64,"):
            #         base64_content = base64_content.split(",")[1]

            #     try:
            #         binary_content = base64.b64decode(base64_content)
            #     except base64.binascii.Error as e:
            #         logger.error("Invalid base64 content")
            #         logger.error(e)
            #         continue

            #     if not binary_content.startswith(b"%PDF"):
            #         logger.error("The decoded content is not a valid PDF")
            #         continue

            #     try:
            #         pdf_data = fitz.open(stream=binary_content, filetype="pdf")
            #     except fitz.fitz.FileDataError as e:
            #         logger.error("Failed to open the PDF document")
            #         logger.error(e)
            #         continue

            #     text = ""
            #     for page in pdf_data:
            #         text += page.get_text()
            #     a["content"] = text
            #     # Cut to the first 50000 characters
            #     a["content"] = a["content"][:50000]
            #     processed.append(a)

        logger.debug(f"Appending {len(processed)} attachments to the chat context")
        self.attachments = processed
