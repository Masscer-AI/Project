# import base64
from openai import OpenAI
import anthropic
from ..logger import get_custom_logger
from types import SimpleNamespace

# import fitz

logger = get_custom_logger("completions")


class TextStreamingFactory:
    client = None
    max_tokens = 4096
    attachments = []

    def __init__(
        self,
        provider: str,
        api_key: str,
        config: dict = {},
        prev_messages=[],
        agent_slug=None,
    ) -> None:
        if provider == "openai":
            self.client = OpenAI(api_key=api_key)
        elif provider == "ollama":
            self.client = OpenAI(api_key=api_key, base_url="http://localhost:11434/v1")
        elif provider == "anthropic":
            self.client = anthropic.Anthropic(api_key=api_key)

        elif provider == "xai":
            self.client = OpenAI(api_key=api_key, base_url="https://api.x.ai/v1")

        self.provider = provider

        self.prev_messages = prev_messages
        self.config = config
        self.agent_slug = agent_slug

    def stream(self, system: str, text: str, model: str):

        if self.config.get("multiagentic_modality", "isolated") == "grupal":
            system = f"{system}\n\nYou must consider that the other AI in the conversation is another user and refer to them as such. ONLY GIVE YOUR RESPONSE."

        if self.provider in ["openai", "ollama", "xai"]:
            return self.stream_openai(system, text, model)
        elif self.provider == "anthropic":
            print("trying to generate with Anthropic")
            return self.stream_anthropic(system, text, model)

        print("PROVIDER NOT FOUND")

    def stream_openai(self, system: str, text: str, model: str):
        reasoning_models = ["o1-mini", "o1-preview", "o3-mini", "o4-mini", "o3", "gpt-5", "o4"]
        is_reasoning_model = model in reasoning_models or model.startswith("gpt-5")

        if is_reasoning_model:
            self.max_tokens = 16000

        messages = [{"role": "system", "content": system}]
        for m in self.prev_messages:
            if m["type"] == "user":
                messages.append({"role": "user", "content": m["text"]})
            elif m["type"] == "assistant":
                multiagentic_modality = self.config.get(
                    "multiagentic_modality", "isolated"
                )

                if multiagentic_modality == "grupal":

                    for v in m["versions"]:
                        if v["agent_slug"] == self.agent_slug:
                            messages.append({"role": "assistant", "content": v["text"]})

                        else:
                            tagged_text = f"This message was generated by other AI in the conversation called: {v['agent_name']} \n\n AI Response: {v['text']}"

                            messages.append({"role": "user", "content": tagged_text})

                else:

                    found_version = next(
                        (
                            item
                            for item in m["versions"]
                            if item["agent_slug"] == self.agent_slug
                        ),
                        None,
                    )
                    if found_version:
                        messages.append(
                            {"role": "assistant", "content": found_version["text"]}
                        )
            elif m["type"] == "prev_ai":
                prev_ai_context = f"""
## Message Type: {m["type"]}

## Explanation
This message was generated by ANOTHER AI previous to you as an answer to the user message. You can this discuss or use this information to generate a better response depending of the conversation context.

## Details
AI_NAME:  {m["agent_name"]}
AI_RESPONSE: 
{m["text"]}
"""

                messages.append({"role": "user", "content": prev_ai_context})

        # Build current user message: combine text + images in one multimodal message
        if len(self.attachments) > 0:
            image_parts = [a for a in self.attachments if "image" in a.get("type", "")]
            if image_parts:
                logger.debug("Appending %d image(s) to user message", len(image_parts))
                content_parts = [{"type": "input_text", "text": text}]
                for a in image_parts:
                    content_parts.append({
                        "type": "input_image",
                        "image_url": a["content"],
                    })
                messages.append({"role": "user", "content": content_parts})
            else:
                messages.append({"role": "user", "content": text})
        else:
            messages.append({"role": "user", "content": text})

        temperature = float(self.config.get("temperature", 1))
        if is_reasoning_model:
            logger.debug("Using reasoning model with temperature 1")
            temperature = 1

        logger.debug(f"Creating response stream with {model}")
        kwargs = {
            "model": model,
            "instructions": system,
            "input": messages[1:],  # keep prior/user turns as input; instructions holds system prompt
            "max_output_tokens": int(self.config.get("max_tokens", 3000)),
        }
        if not is_reasoning_model:
            kwargs["temperature"] = temperature
            kwargs["top_p"] = float(self.config.get("top_p", 1.0))

        with self.client.responses.stream(**kwargs) as stream:
            # SDK compatibility: some versions expose text deltas as events only.
            if hasattr(stream, "text_deltas"):
                for delta in stream.text_deltas:
                    if delta:
                        yield delta
            else:
                for event in stream:
                    if getattr(event, "type", None) == "response.output_text.delta":
                        delta = getattr(event, "delta", "")
                        if delta:
                            yield delta

            final = stream.get_final_response()
            usage = getattr(final, "usage", None)
            if usage:
                yield SimpleNamespace(
                    completion_tokens=getattr(usage, "output_tokens", 0),
                    prompt_tokens=getattr(usage, "input_tokens", 0),
                    total_tokens=getattr(usage, "total_tokens", 0),
                )

    def stream_anthropic(self, system: str, text: str, model: str):
        messages = []
        multiagentic_modality = self.config.get("multiagentic_modality", "isolated")

        for m in self.prev_messages:
            if m["type"] == "user":
                messages.append({"role": "user", "content": m["text"]})

            elif m["type"] == "assistant":
                multiagentic_modality = self.config.get(
                    "multiagentic_modality", "isolated"
                )

                if multiagentic_modality == "grupal":

                    for v in m["versions"]:
                        if v["agent_slug"] == self.agent_slug:
                            messages.append({"role": "assistant", "content": v["text"]})

                        else:
                            tagged_text = f"This message was generated by other AI in the conversation called: {v['agent_name']} \n\n AI Response: {v['text']}"

                            messages.append({"role": "user", "content": tagged_text})

                else:

                    found_version = next(
                        (
                            item
                            for item in m["versions"]
                            if item["agent_slug"] == self.agent_slug
                        ),
                        None,
                    )
                    if found_version:
                        messages.append(
                            {"role": "assistant", "content": found_version["text"]}
                        )

            elif m["type"] == "prev_ai":
                prev_ai_context = f"""
## Message Type: {m["type"]}

## Explanation
This message was generated by ANOTHER AI previous to you as an answer to the user message. You can this discuss or use this information to generate a better response depending of the conversation context.

## Details
AI_NAME:  {m["agent_name"]}
AI_RESPONSE: 
---
{m["text"]}
---
"""
                messages.append({"role": "user", "content": prev_ai_context})

        messages.append({"role": "user", "content": text})

        with self.client.messages.stream(
            max_tokens=self.max_tokens,
            system=system,
            messages=messages,
            model=model,
            temperature=self.config.get("temperature", 0.5),
        ) as stream:

            for text in stream.text_stream:
                yield text

    def process_attachments(self, attachments=[]):
        logger.debug(f"Processing {len(attachments)} attachments")

        processed = []

        for a in attachments:
            if "image" in a["type"]:
                processed.append(a)

            # elif "audio" in a["type"]:
            #     processed.append(a)
            # elif "pdf" in a["type"]:
            #     base64_content = a["content"]
            #     if base64_content.startswith("data:application/pdf;base64,"):
            #         base64_content = base64_content.split(",")[1]

            #     try:
            #         binary_content = base64.b64decode(base64_content)
            #     except base64.binascii.Error as e:
            #         logger.error("Invalid base64 content")
            #         logger.error(e)
            #         continue

            #     if not binary_content.startswith(b"%PDF"):
            #         logger.error("The decoded content is not a valid PDF")
            #         continue

            #     try:
            #         pdf_data = fitz.open(stream=binary_content, filetype="pdf")
            #     except fitz.fitz.FileDataError as e:
            #         logger.error("Failed to open the PDF document")
            #         logger.error(e)
            #         continue

            #     text = ""
            #     for page in pdf_data:
            #         text += page.get_text()
            #     a["content"] = text
            #     # Cut to the first 50000 characters
            #     a["content"] = a["content"][:50000]
            #     processed.append(a)

        logger.debug(f"Appending {len(processed)} attachments to the chat context")
        self.attachments = processed
